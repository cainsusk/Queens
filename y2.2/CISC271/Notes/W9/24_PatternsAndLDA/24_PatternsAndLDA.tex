\documentclass[12pt]{book} 

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{import}
\usepackage{amsfonts}
\usepackage{booktabs}

\setlength{\parindent}{0em}  % sets auto indent at new paragraph to none

\newcommand{\incfig}[1]{%
        \import{./figures/}{#1.pdf_tex}
}

\newcommand{\incimg}[2]{%
       \begin{figure}[h]
               \centering
               \includegraphics[scale = #2]{./figures/#1}
       \end{figure}
}

\title{\coursetitle\linebreak\lecturename}
\author{\\Cain Susko\\ 
           \\ \\ \\
      Queen's University 
    \\School of Computing\\} 

%=-=-=-=-=-title-=-=-=-=-=%
\newcommand{\lecturename}{Patterns - Linear Discriminant Analysis}
\newcommand{\coursetitle}{Linear Data Analysis}
%=-=-=-=-=-#####-=-=-=-=-=%

\begin{document}
\begin{titlepage}
        \maketitle
\end{titlepage}


\section*{a Finding Patterns in Labeled Data}
This section will focus on how to adapt PCA to analyze data that has labels. The main conectps are
what does PCA do when we expolre is on the means of the labeled data; how does PCA and the Rayliegh Quotient help us sove this problem; what is LDA - optimization.

\paragraph{What can PCA Do?}
PCA is a way of explaining, not predicting data. PCA scores are used to reduce the dimensionality
of a dataset. Clustering will often work better after PCA.

\section*{b Example of PCA for Labeled Data}
given the data generated by prof. Ellis: 300 points in data matrix $A$. the datapoints are 
inside 3 elipses, where 2 are merged. The labels used are $y_j \in \{-1, +1\}$.
We must find the mean, scatter matrix, and PCA of the data and. Additionally, we must
Carry the labels from the data onto the loading vectors and designate a colour to each 
label for visual assesment.
The labeled, coloured data is the following:
\incimg{data}{0.5}

After PCA, the data look like so:
\incimg{dataPCA}{0.5}

where the x inidcates the mean of all of data, the large arrow indicates the first loading vector
and the smaller arrow indicated the second loading vector. Thus, we can see that the loading 
vectors are aligned with the major and minor axis of the original data.

\paragraph{Projection}
When we then project the labels onto these axis (the 2 loading vectors) we can see that the second
loading vector may actually be doing a better job than the first loading vector at distinguishing 
the labels.
\incimg{project}{0.5}

\paragraph{PCA on the Mean}
If we indicate the mean of the red and cyan clusters (as x's in their respective colours)
and then perform PCA on the means of the data we get the magenta axis which is directed 
between the 2 means and goes through the means of the overall data.

\paragraph{Data With Labels: So Far}
\begin{itemize}
        \item we can colour data labels using the \texttt{gsatter} function.
        \item doing PCA with a scatter matrix is finding the maximum variance which is the 
                first loading vector from the PCA.
        \item PCA treats all data uniformly
\end{itemize}
\pagebreak

\section*{c Fishers Linear Discriminant}
there are many types of scatter matricies:
\begin{itemize}
        \item[$S_t$] Total scatter = $M^\top M$
        \item[$S_w$] Scatter within labels = $S_1 + S_2$
        \item[$S_b$] Scatter between labels = $M_b^\top M_b$
\end{itemize}
\incimg{scatterMat}{0.7}

We thus have 2 objectives:
\[\vec v_{max} \in S_B = \max_{\vec u \in \mathbb{R}^n}R(S_B)\]
\[\vec v_{min} \in S_w = \max_{\vec u \in \mathbb{R}^n}R(S^{-1}_w)\]

Thus, Fishers Linear Discriminant (LDA) is:
\[\vec v = \max_{\vec u \in \mathbb{R}^n} S_w^{-1}S_B\]

therefore, the result (maximum eigenvector within $S_w^{-1} S_B$) is Fishers Linear
Discriminant that simultaneously maximizes the between label scatter and minimizes the
within label scatter.
\pagebreak

\section*{d Example of LDA for Labeled Data}
given the data with 2 labels and their means (cyan and red x's) 
\incimg{data2}{0.5}

after finding the minimum PCA Axis (cyan, the axis that minimizes the within label scatter) 
and the maximum PCA axis (magenta, the axis that maximizes the between 
label scatter), which goes through the 2 means of the data.
\incimg{data2PCA}{0.5}

When we use Linear Discriminant Analysis, we get the white line, which simultaneously maximizes
the between label scatter and minimizes the within label scatter.
\incimg{data2LDA}{0.5}
\pagebreak

when we then project the labels to the LDA axis we can see that the line seperates the
2 labels very well:
\incimg{data2project}{0.5}

we can also derive a seperating hyper plane from the LDA Axis by choosing a point
along the Axis that maximizes the accuracy of the seperation for
the 2 labels within the data.
\incimg{data2part}{0.5}

The hyper-plane will be orthogonal to the LDA axis.

\section*{d Example of LDA for Iris Data}
\paragraph{Recall}
The Data is made up of 2 measurments: petal length and width. The labels we will use 
are:
\begin{itemize}
        \item[\textbf{B}] Beach-Head plant
        \item[\textbf{P}] Purple plant
\end{itemize}
\pagebreak

when we plot these with width as the y axis and length as the x axis:
\incimg{irisData}{0.5}

After applying LDA to the iris data we get the LDA axis as the arrou and the hyper-plane
of seperation is the best seperation of the 2 labels orthogonal to the LDA axis.
\incimg{irisLDA}{0.5}

while there is still an anomalous plants, the hyper plane is a much better way of seperating 
the 2 labels than say, k-means clustering.

\subsection*{LDA: HyperPlane From Axis}
\begin{enumerate}
        \item Project onto axis $\vec w = \vec v_{max}(S_w^{-1} S_B)$
        \item recall: hyperplane needs bias scalar $b$
        \item Thus: there will be further optimization with $b$, possibly using 
                an ROC curve (covered in the next lesson)
\end{enumerate}

\section*{Learning Summary}
Students should now be able to:
\begin{itemize}
        \item compute matricies from labeled data
        \item compute fishers discriminant
        \item project labels onto LDA axis to verify if LDA is working as intended
\end{itemize}







\end{document}

