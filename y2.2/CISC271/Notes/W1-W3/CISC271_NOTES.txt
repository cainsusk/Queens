
CISC271 linear data analysis NOTES
================================== FONT must be MONOSPACE
look into doing this: [https://castel.dev/post/lecture-notes-1/]


######
WEEK 1
######


W1-mon
======
make sure to check the Materials Widget on OnQ homepage

there is also a website that has all the course material:
https://research.cs.queensu.ca/home/cisc271/lectures.html

Assignments should be given back the following monday

Quizzes are based on Matlab and you will be using or modifying your existing code from the-
preceeding Assignment in real-time

make sure to look at the 'report guide for students' in week 1 on OnQ
also, dont submit a .zip file for the Assignments


W1-tue
======
this is a MATLAB TUTORIAL [see Documents/MATLAB/inClass_scripts/tue_C2 for the code]

matlab indexing is:     Matrix(row, col)

the workspace contains the working variables
the command window is self explanatory
the editor contains the script editor where the main code is
use 'clc' to clear the Command Window and use 'clear' to clear the Workspace

matlab uses 1 based indexing (everything starts couting from 1 by default)

#INTRODUCE SCALAR VALUE
if you intorduce the scalar variable 'a' in the Editor and run the script a will show up in the Workspace
you can also declare scalar variables using operations (ie. sum = 5 + 5;)

if you then type 'a' into the Command Window it will print the value assigned to a in the script
additionally, if you dont put a semicolon ';' at the end of the 'a' declaration, it will print out when you run the script
	semicolons are only needed if you dont wanna print a variable or operation (like when the result is HUGE)
furthermore, disp(a) displays 'a' in a nicer, more readable way

#INTRODUCE VECTOR VALUE
you can declare a vector (row or column):
	row = [1,2,3,4];
	col = [1;2;3;4];
semicolons seperate rows in a matrix, commas seperate columns

#VECTOR OPERATIONS
you can add vectors like so:
	new_vec = [1,2,3,4] + [1,2,1,1];
to add properly the vecotrs MUST have the same dimensions

#TRANSPOSE VECTOR
either use VECTOR' or transpose(VECTOR)

#MATRICIES
make a matrix by seperating columns by spaces and rows by semicolons ';'
	mat = [1 2 3; 4 5 6; 7 8 9]

to access a matrix it is similar to slicing in Python
	mat(2,2) == [5]     (row 2, col 2)
	mat(1,:) == [1,2,3] (':' means the whole row)
	mat(:,2) == [2;5;8] (':' means the whole col)
	mat(1,1:3)= [1,2,3] (row 1, column 1 to 3)
      mat(2,1:end)= [4,5,6] (row 2, column 1 to end)
      mat(1,[1,3])= [1,3]   (row 1, column 1 & 3)

how to multiply a Matrix by a vector
	mat_mul      == mat  * [2;3;2];	retunrs a vector, normal matrix multiplication
	mat_elem_mul == mat .* 3;	returns a matrix where each element has been multiplied by a scalar value (3) 

#IF STATEMENTS
thyere basically just like from python accept without the ':'s

var = 1;
if var == 1
	disp("hello")
elseif var == 3
	disp("hola")
else	
	disp("bye")
end

#SWITCH STATEMENT
similar to the switch statement in C (although im sure its in other languages too)

m = "feb";
switch m
	case "feb"
		disp("happy febuary")
	case "jan"
		disp("happy january")
end

#FOR LOOPS
similar to Python again but without brackets (mom just have me an icy square :) )
this loop has a start of 1, a stop of 10, and a step of 2

for idx = 1:10:2
	disp(idx)
end

the following loop iterates through a list

list = [1 2 3 4]
for list_val = list
	disp(list_val)
end 

#WHILE LOOPS
very basic loops

found = false;
target = 5;
i = 1;
while found == false
	if list(i) == target
		found = true
		disp("Found")
	end
	i = i + 1

	if i == 5 %the len of list is 5
		disp("not Found")
		break %stops loop
	end
end

note that the 'list' referenced in this and previous examples is actually a 'row vector' in matlab

#PLOTTING FIGURES & GRAPHS
the function 'linspace(a,b,n)' will generate 'n' values from 'a' to 'b' numerically

now we can plot values generated from linspace using plot & figure declarations:

x = linspace(1,20,21);
y = linspace(1,40,21);
figure;
	plot(x,y,"r*")
	xlabel("x-axis")
	ylabel("y-axis")
	title("Plot :)")
	legend()

you can save plots by going into the file menu in the plot and save as

#FUNCTIONS
you can create and use functions just like most other programming languages
they are in the format of
	function "return variable" = "function name"(values)
thus:

function [addition, temp] = sum(a, b)
	addition = a+b;
	temp = 1;
end
%we can call it like so
[add, tmp] = sum(2,3)
disp(add)


1a-intro
========
using vectors and matricies, hence linear
using data, real and fake
thus, we're analyzing this linear data

we will be using matlab for parts of this course

some basic lin.alg examples:
Eigen Vector
	is the matrix such that:
	Av = {LAMBDA}v

Hilbert Matrix
	is a matrix where each element is:
	a = 1/[i+j-1]
	where i and j are a's indexes


1b-class logistics
==================
see {mon W1}


1c-vectors withim matricies
===========================
alsways use: What Would Einstein Do? (WWED)

also, VECTORS are denoted by V (and ∠), for example: [∠x] is the VECTOR x

consider: 2x2 matrix:

	A ∈ R^[2×2]

and a vector with 2 entries:

	∠c ∈ R², ∠w ∈ R²
thus:
	∠c = A*∠w
  [c1;c2] = [a11, a12; a21, a22] * [w1; w2]
	  = [a11*w1 + a12*w2; a21*w1 + a22*w2]
	  = w1*[a11; a21] + w2*[a12; a22]
	  = w1*∠a1 + w2*∠a2	where a1 and a2 are column vectors of A

for matrix multipication the columns of the first multiple (X*y) must equal the number of rows in the second multiple (x*Y)

consider that any 2 vectors with a 'weight' or multiple can equal any vector on the plane:
	∠c = w1*∠a1 + w2*∠a2
thus any vector c can be made from the sum of the weighted vectors a1 and a2

this can also be extended to 3-space such that:

	∠c = w1*∠a1 + w2*∠a2 + w3*∠a3
this can be extended to n-dimensions


1d-EigenFacts
=============
something thats true and should always be assumed as such (within the scope of this course)

consider:
	the vector v [∠v]
	the matrix A [A]
such that:
	A*∠v = ∠u

if vector u is linear dependent to vector v it is an EigenVector: 	(linear dependence is when a set of vectors are multiples of eachother)
	A*∠v = λ*∠v							(lmda = lambda, lmda*V is an EigenVector)

Properties of EigenValues:
for any square matrix [A] 			(A[in]R^[nxn])
EigenValues are contained in the set [L(A)]:	(where L(A) is the set of matrix A's EigenValues)

	matrix [A] has [n] eigenvalues, possibly complex, not distingct
        symmetric matrix [B] has [n] real number EigenValues
	skew-symmetric matrix [S] has [n] EigenValues, either zero or purely imaginary
	Triangular matrix [T] has all the diagonaly values in L(T) [t_ii[in]L(T)] where i are the values from 0 to the largest index in the matrix
	if 2 matricies [A,C] are similar (related by an inveratable matrix), then [L(A)=L(C)]
		because trace and determinant are derived from EigenValues, these also hold for similar Matricies:
		[tr(A)==tr(C) && |A|==|C|]


2a-intro to graphs
==================
Graphs are composed of corners (verticies) and edges [G(V,E)]
an adjacentcy matrix for a graph [G] is [A(G)], which is symmetric and binary	(binary, elements are either [0,1])

how can we represent a graph as a matrix?
	A graph is a mathematical structure of:

	Set [vT] of vertecies. NONEMPTY and FINITE, elements are [v_j]
	Set [E]  of edges. no repetitions or ordering: [e_k = {v_i*v_q} or = (v_i*v_q)=(v_q*v_i)]

	the graph [G] is represented as [G(vT, E)]
		G = {{1 2 3 4 5},(23),(12), (45)}	(edge from 2-3, 1-2, 4-5)


2b-definitions for graphs
=========================
consider:
	G# = {{1 2 3 4 5},(12),(23),(24),(45),(46)}

	(1)--(2)--(4)--(6)
	    /	     \
	  (3)	     (5)

definitions:
	incident	a vertex and edge are incadent if theyre adjacent (connected)
			an edge is incadent to another edge if they share a vertex	
			a vertex is incadent to another vertex if they share an edge

	degree		the degree of a vertex is the number of edges incadent (connected) to is

	subgraph (sG)	a graph [G] is a subgraph if and only if it is contained within another graph [H] such that:
				[vT_G ∈ vT_H] [AND] [E_G ∈ E_H]

	bipartite	a graph [G(vT, E)] is bipartite if [vT] can be partitioned into 2 sets [vT_L, vT_R] such that:
			each edge can be written as [e = (v_i, v_j)] where
				[ v_i ∈ vT_L ] ∧ [ v_j ∈ vT_R ]

			if you were to pick the graph up from one of its vertecies, it would make / be a tree. 
			[G#] is an example of a bipartite graph

	path		a finite sequence of incadent edges from [v_i->v_j]

	component sG	a subgraph created from cutting an edge that results in a subgraph that has no other paths to the rest 
			of the graph barring the edge that was cut.
			we could take [G#] as an example:

				(1)--(2)--(4)--(6)
				    /	     \
			 	  (3)	     (5)

			if we then cut (24):

				(1)--(2)   (4)--(6)
				    /	     \
				  (3)	     (5)

			we get 2 component subgraphs:
				G1 = {{1 2 3},(12),(23)}
				G2 = {{4 5 6},(45),(46)}

			in other words, there should be a path to any part in a component subgraph from any other vT of-
			the component subgraph
			additionally,   there should be NO path from a part of the component subgraph to any part not-
			within the component subgraph

NOT allowed:
	multiple edge	2 distingct edges [p,q] that both start from the same vertex and both end at the same vertex
			e_p = e_q

	loop		an edge that starts and ends at the same vertex
			e = (v_i, v_i)
	
	pseudographs	set of vertecies and edges that has atleast one multiple edge and/or loop 

				
2c-Adjacency Matrix
===================
let [G(vT,E)] have n Vertecies, m Edges
Adjacecy Matrix:
	[A(G)] is defined as a matrix with [m x m] dimensions such that:

		/ 1 if (v_i,v_j)  ∈ E 
	 a_ij ==
		\ 0 if (v_i,v_j) ¬∈ E						('¬' means NOT)

	thus [A] is binary and, because [(v1 v2)=(v2 v1)], A is also symmetric.
	[A] is always real and always has a 0-diagonal.
	[A] always has only real EigenValues

consider:
	G1 = (1)---(2) (4)
	       \   /     \
	        (3)      (5)

	A(G1) = 			each row corresponds to a vertex, the values in the row correspond to each vertex 
	(1)	/0 1 1 0 0\		in the graph, 1 if the row-vT and value-vT are incident. 0 otherwise
	(2)	|1 0 1 0 0|
	(3)	|1 1 0 0 0|
	(4)	|0 0 0 0 1|
	(5)	\0 0 0 1 0/
	
	G# = (1)--(2)--(4)--(6)
	          /      \
	        (3)	 (5)

	A(G#) = 
		/0 1 0 0 0 0\
		|1 0 1 1 0 0|
		|0 1 0 0 0 0|
		|0 1 0 0 1 1|
		|0 0 0 1 0 0|
		\0 0 0 1 0 0/

ones vector:
	is defined as a vector where each entry is 1
		∠1 = {1;1;...;1}
	it can count the number of 1's in a binary matrix

degree vector:
if you multiply an Adjacency marix [A(G)] by the ones vector, the product will be the degree vector of [G]

	A(G1)*∠1 = [2;2;2;1;1]
	A(G#)*∠1 = 
		   /0 1 0 0 0 0\   /1\
		   |1 0 1 1 0 0|   |1|
		   |0 1 0 0 0 0| * |1|
		   |0 1 0 0 1 1|   |1|
		   |0 0 0 1 0 0|   |1|
		   \0 0 0 1 0 0/   \1/

		 = [1;3;1;3;1;1]
thus this is the degree vector for [G#]


2d-non bipartite braphs
=======================
consider:
	G3 = (1)-(2)  (4) (5)
	       \ /	\ /
     	       (3)	(6)
	
	there are 2 component subgraphs [sL,sR] within this graph
	while sR may be bipartite, sL and thus G3 is not

	A(G3) = 
		/0 1 1 0 0 0\
		|1 0 1 0 0 0|
		|1 1 0 0 0 0|
		|0 0 0 0 1 0|
		|0 0 0 1 0 1|
		\0 0 0 0 1 0/

	we can see the 2 discrete components of the graph within the matric by the groups of 1's

consider:
	G4 = 
	     (1)-(2)-(4)-(5)
	       \ /     \ /
     	       (3)     (6)

	A(G4) = 
		/0 1 1 0 0 0\
		|1 0 1 1 0 0|
		|1 1 0 0 0 0|
		|0 1 0 0 1 0|
		|0 0 0 1 0 1|
		\0 0 0 0 1 0/

2-learning summary
------------------
we should now be able to:
	characterize a graph
	make an Adjacency matrix from a specification or diagram
	draw a graph from an Adjacency matrix
	
	try:
		coding an edge list to adjacency matrix


3a-degree matrix
================
recall:
        the degree of a vertex is the number of edges the vertex is connected to
        the degree vector is the product of [A(G)*∠1 = d∠]

we can use [d∠] to construct the degree matrix:
	D(G) = diag(d∠) =

		/d1 0   0 ...  0\
		|0  d2  0 ...  0|
		|...............|
		\0  0   0 ... dn/

the diagonal is made up of the entries from the degree vector [d∠]
by construction this matrix is positive, semidefinite, and symmetric


3b-Laplacian matrix
===================
comes from the Adjacency & Degree matrix and describes both

laplacian matrix:
	L(G) = D(G)-A(G)	(D has 0-values accept for on diagonal, A has 0-values only on diagonal)
	by construction, L is symmetric

steps of algorithim for solving (by hand):
{1}	write [A(G)] without entries on the diagonal (theyre all zeros by definition anyways)
{2}	negate (multiply by -1) all non-zero entries
{3}	replace diagonal entries [a_ii] with [d_i ∈ d∠]

consider:
	G# = (1)--(2)--(4)--(6)
	          /      \
	        (3)	 (5)	
step {1}
	L1 =
		/  1 0 0 0 0\
		|1   1 1 0 0|
		|0 1   0 0 0|
		|0 1 0   1 1|
		|0 0 0 1   0|
		\0 0 0 1 0  /
step {2}
	L1 =
		/   -1  0  0  0  0\
		|-1    -1 -1  0  0|
		| 0 -1     0  0  0|
		| 0 -1  0    -1 -1|
		| 0  0  0 -1     0|
		\ 0  0  0 -1  0   /
step{3}
	L1 =
		/ 1 -1  0  0  0  0\
		|-1  3 -1 -1  0  0|
		| 0 -1  1  0  0  0|
		| 0 -1  0  3 -1 -1|
		| 0  0  0 -1  1  0|
		\ 0  0  0 -1  0  1/
thus, we have found the laplacian matrix of [G#]

consider:
	G1 =    (1)-(2) (4)
	          \ /     \
	          (3)     (5)

	L1 = 
		/ 2 -1 -1  0  0\
		|-1  2 -1  0  0|     / L_b1    0 \
		|-1 -1  2  0  0| ==  |           |	(where [L_b] is a block in the L matrix)
		| 0  0  0  1 -1|     \ 0    L_b2 /
		\ 0  0  0 -1  1/

consider:
	A(G#) = 
		/0 0 0 1 1 1\
		|0 0 0 0 0 1|    / 0        A_b \
		|0 0 0 0 0 1| == |              |	(where the block [A_b^T] is a transposed version of [A_b])
		|1 0 0 0 0 0|    \ [A_b]^T    0 /
		|1 0 0 0 0 0|    
		\1 1 1 0 0 0/

	NOTE: [G#] is a bipartite graph. every bipartite graph can be represented to have this form of Adjacency matrix

in summary:
	the Laplacian Matrix is both: symmetric & real
	
	additionally it is Diagonally Dominant (d.d), which means:
		for every row, the sum, excluding the diagonal entry, of the row must be less than or equal to the diagonal entry
		l_ii >= SUM_[j/=i](|l_ij|)			NOTE: it is the sum of the ABSOLUTE values

	the Laplacian matrix (and any d.d matrix) is Positive Semidefinite:
		where every EigenValue is non-negativem [EigenValues >= 0] 

	observe:
		L*∠1 = (D-A)*∠1 = d∠-d∠ = ∠0
	thus:
		∠1 is an EigenVector &
		0 is its EigenValue


3c-Laplacian EigenValues
========================
getting EigenValues from L:

	G1 =    (1)---(2) (4)
	          \   /     \
	           (3)      (5)

	L1 = 
		/ 2 -1 -1  0  0\
		|-1  2 -1  0  0|     / L_b1    0 \
		|-1 -1  2  0  0| ==  |           |	(where [L_b] is a diagonal block in the L matrix)
		| 0  0  0  1 -1|     \ 0    L_b2 /
		\ 0  0  0 -1  1/

using MATLAB we can find the EigenValues [Ev] of L1:

	Ev1 = 0
	Ev2 = 0
	Ev3 = 2
	Ev4 = 3
	Ev5 = 3
from these Eigen Values we can derive:
	
	dimension of nullspace of L1	= 2	(2 eigenvalues are 0)
	dimension of Eigenspace:
			 Ev3		= 1	(1 eigenvalue is 2 [Ev3=2])
			 Ev4,5		= 2	(2 eigenvalues are 3)

observing EigenVectors:
	positive and negative values in an EigenVector derived from [L(G)] correspond to visual clusters in [G]

	the entries in the EigenVector derived from [L(G)] that have the same sign [+,-] correspond to
        respective clusters of nodes in [G]

	
3d-Laplacian matrix: Feilder vector
===================================
used to find the binary clustering of vertices

steps for algorithim to find Feilder vector:
{1}	create [L(G)] of graph [G]
{2}	compute the second EigenVector [Ev2] of [L(G)]
{3}	assign the negative entries of [Ev2] to a Set 1
{4}	assign the positive entries of [Ev2] to a Set 2

Laplacian matrix properties:

	- Feilder vector is a binary clustering of verticies, derived from the EigenVectors of L
	- symmetric, postitive semidefinite
	- ones vector [∠1] is the EigenVector of 0-EigenValue
	- dimensions of nullspace is equal to the number of graph components
	- for [k] components:
		> it is similar to a matrix with k diagonal blocks (see {3c} for example)

3-learning summary
------------------
we should now be able to:
	find the degree of a vertex in a graph
	create Laplacian matrix [L(G)]
	compute the # of graph components
	compute a binary clustering of vertecies
	
	try:
		program to find clusters from Adjacency matrix [A(G)]


######
WEEK 2
######


W2-mon
======

|MatLab control flow:
        - logical expressions
                > logic operators
                > relation operators
                > operation precedence (binding order)
        - if/else
        - loops
                > while
                > for

        matlab is dynamically typed !

        logical expressions:
                these have 2 types "logical 0" & "logical 1"
                                   "True"        "False"
                the Logical operators are:
                        &       and
                        |       or
                        ~       not
                operation precedence:
                        1)      ~
                        2)      &
                        3)      |
                MatLab automatically converts from "real" to "logical"

                Relational Operators are:
                        equality        ==    
                                        ~=
                        inequality       >
                                        >=
                                         <
                                        <=

                the Logical operators can combine the results of the realtional operations
                        a ~= b  ≡  (a<b)|(a>b)


        if/else:
                in matlab there is no then expression thus the syntax is:
                
                if statement:
                        
                        if 'logical expression'
                                statement(s):
                        end 

                if/else:
                        
                        if 'logical expression'
                                statement(s)
                        else
                                statement(s)
                        end

        loops:
                while loop:
                        
                        while 'logical expression'
                                statement(s)
                        end
                
                        break will stop the loop and continue on after 'end'

                        while 'logical expression'
                                statement(s)
                                break;
                                statement(s)
                        end

                for loops:
                        Warning!!MatLab Pre-Defines 'i' and 'j'
                                                    √-1     √-1

                        use something other than i and j for index values, can use [h,k] instead
                        
                        for 'index' = 'values'
                                statement(s)
                        end

                        for 'index' = 1:n
                                statement(s)
                        end

                        for 'index' = 1:array
                                statement(s)
                        end

|4×4 Matricies:

        random number generator:
                rng('default')          [sets the rng to default mode]

        declare matrix:
                A = randi(10, 4)        [makes a 4×4 matrix from random ints]
                
        get EigenValues from A:
                [Evecs, Evals] = eig(A, 'vector')
                
        to print each value from Evals one can use:

                for k = 1:4
                        disp(Evals(k))
                end

        to print the EigenVectors from the matrix Evecs:

                for k = 1:4
                        disp(:,k)
                end
                
W2-tue
======
the assignment consists of 2 parts:
        the matlab code                 definitely check the templeates for these on OnQ
        the PDF report

the assignmnet this week is on grouping a given graph into clusters using the binary Association Matrix

in order to load data into a matrix you can use:
        load(data)

4a-Vector Space
===============

|what is a vector space:
        suppose we are given 3 vecotrs:
                ∠a₁ ∠a₂ ∠a₃ 
        what space do they reside in ?

        we will use ∥V ∥W ∥U to represent spaces
        they can be defined constructuvely or by restriction
                what is in it or what isnt in it

        real numbers is written as |R
        vector with m entries is size m
        written as:
                Vv IN |R^m

        the plane is |R² 

4b-Space Definitions
====================

|Partition:
        the partition of set |S
        non-empty sets |P_n
        the union of |P_n is IN |S
        n ‡ j then |P_n ∩ |P_j = {}

        example:
                Mat IN |R^[4×4]
                partition Mat = [a, b; c, d]
                consider the partition of Mat into columns,
                each column is a vector block partition

                suppose A has m rows and n columns
                        A IN |R^[m×n]
                        A = [∠a₁ ∠a₂ ... ∠a_n] with ∠a_i IN R^m
                        
                        ∠a_i IN |R*m, w_i IN |R, ∠c = R^m

                        w₁∠a₁+w₂∠a₂+...+w_n*∠a_n = ∠c

                        A*∠w = [∠a₁ ... ∠a_n][w₁;w₂;...;w_n]


4c-Vector space Properties
==========================

abstract:
        1) a field |F, usually |R                               (can be complex |C or rational |Q as well)
        2) a set of vectors |V. 2 operators, with closure       (c IN |F AND ∠u IN |V) → c*∠u IN |V

interperetation:
in the sense of logic
        For any u and v that are in the set |V and any a and b that are in |R then a times u plus b times v is in |V
                ∠u, ∠v In |V → a*∠u + b*∠v IN |V

        this is both communative and distrubutive

        NOTE: always have ø IN |V


4d-Vector subspace
=================
this is usually what people mean when they say vector space.

subspace is a linear subset meaning it is a multiple of the space and it is in the space 

Dimension of vector space:
        the minimal number [k] of vectors needed to construct space |V


4e-Null Space
=============
the nullspace is the vector that when multiplied by the matrix it is the null space of, the product is the zero vector
        A * null(A) = ∠0

recall:
        the row echelon form 
        - each pivot is the first non-0 entry in row
        - below each pivot is 0

        reduced row echelon form (RREF)
        - each pivot has been modified by row operations to be 1
        - above each pivot is 0

consider:
            / 3  3  9  6\              /1 0 2 0\
        A = | 1 -1  1  1|  -RREF-> R = |0 1 1 0|
            \ 2  2  6  2/              \0 0 0 1/

        null(A) = [-2 -1 1 0]

FACT: we can always use the product of the RREF [R] multiplied by a perutation matrix [P] (which has only 1s and 0s and det=1)
        to get the Identity [I], some factor [F], and 1 or more rows of 0s
                R*P = [I, F; 0, 0;]
        thus the null space of any matrix is:              
                null(A) = P * [-F; I]

if null(A) is trivial   (has only ∠0 is it) then:
        A*∠w = ∠b has 1 solution

if null(A) is NOT trivial then:        
        subspace of |W of vectors ∠w IN R^n
        such that A*∠w = ∠0

consider:
        |V = all ∠u = [x; 2x] IN |R²

        all ∠u IN |V are ∠u*∠c=0 where ∠c = [2; -1]

        ∠u * ∠c = ∠c * ∠u = ∠c^T * ∠u

        thus:
        A = ∠c^T = [2, -1]
        rref(A) = [1, -1/2]
        null(A) = [1/2; 1] or [1; 2]x
        

5a-intro to basis vectors
=========================

|spanning set:
        a spanning set or span of |V is a set of vectors that can be linearly combined to form
        every possible vector in a vector space |V

        there is NO restriction to the number of vectors in a span (accept at least 1)

        properties:
                Closed:
                for any real numbers w₁ w₂ ... we have w₁∠a₁+w₂∠a₂+... IN |V

                Complete:
                for any vector c in vector space |V there exists real numbers w₁ w₂ ... such that 
                ∠c = w₁∠a₁+w₂∠a₂+...

|linear independence:
        2 vectors are linearly dependent if there are numbers that each could multiply the 2 vectors in
        question such that their sum would equal 0:
                w₁∠a₁+w₂∠a₂+... = ∠0 

        in other words, we can represent the vectors that are linearly independent, with eachother (theyre
        multiples of eachother)


5b-basis vector definitions
===========================
consider:
        given vector a [∠a] that spans |V
        a basis of |V is a linearly independent spanning set:
                {[1;0] [0;2]}           basis
                {[1;0] [1;0.0001]}      basis
                {[1;1] [-2;-2]}         NOT
                {[1;1] [0;0]}           NOT
                {[1;0] [0;2] [a;b]}     NOT

Method:
        gather ∠a into A
        For R = rref(A):
                each pivot of R indicates a linearly independent vector of A in vector space |V
                (this is whats known as the comlumn space)

examples:
        A = [1,2,6;1,-2,2]
        R = [1,0,4;0, 1,1]

        thus:
                {[1;1] [2;-2]}          basis

5c-basis of Matrix
==================
let A I> R^[m×n]                        NOTE [IN == I>] 

Facts:
        the rank of A is:
                the number of linearly independent rows
        full rank:
                rank(A) = min(m,n)
        if all vectors ∠a_i are linearly independent then:
                rank(A) = n

        if n > m then:
                ∠a_i are linearly dependent

        if null(A) is trivial then:
                ∠a_i are linearly independent
        if null(A) is non-trivial then:
                ∠a_i are linearly dependent
        
        A is invertable 
        
        columns [∠a] are a BASIS for R^m

consider:                                               

        a set of [n] vectors:
                ∠v_i I> R^m they are linearly independent
        gather as A = [∠v₁ ∠v₂ ... ∠v_n]
        for any non-zero ∠c I> |V
        there is a unique linear combination:
                ∠c = w₁∠v₁+w₂∠v₂+...+w_n * ∠v_n
        there is a unique solution to:
                ∠c = A*∠w
        
        thus, given an arbitrary vector non-zero [∠c I> |V] there is a unique set of weights [∠w] such that 
        the product of [A] and [∠w] is the vector [∠c]


5d-orthoganol spaces
====================
given ∠u and ∠v
if they are orthogonal then:
        ∠u·∠v   = 0
        ∠u^T*∠v = 0

consider:
        the basis vectors ∠u_i

        Orthogonal basis:
                (i ‡ j) → (∠u_i · ∠v_j =0)

        OrthoNormal basis:
                both Orthogonal and 
                ∠u_i·∠u_i = 1

        {[1;0;0] [0;0;1]}       OrthoNormal
        {[1;0;-1] [1;0;1]}      Orthogonal 
        {[1/√2;0;-1/√2] 
         [1/√2;0; 1/√2]}        OrhtoNormal

|basis vectors for multiple spcaes:
        considering the vector spaces |U and |V
        suppose (∠u ≠ ∠0) ۸ (∠u I> |U)
                is true if and only if [∠u ~I> |V]

        if this holds, then |U and |V are Orthogonal Compliments

|dot product [·] Rule:                          NOTE [Vv == ∠V]
         for [∠u ≠ ∠0] and [∠v ≠ ∠0]
                (∠u ∈ ∥U) ∧ (∠v ∈ ∥V) → (∠u·∠v = 0)

        example:
                in R³ ∥U is the z-axis, thus [∠u ∝ [0;0;1]] u is proportional to the vector
        
                a basis for ∥V is:      /1\     /0\
                                        |0|     |1|
                                        \0/     \0/

                thus if there is a multiple [w] that satisfies:
                        (w₁∠v₁ + w₂∠v₂)·∠u = 0
                then ∥U and ∥V are orthangonal Compliments 

6a-Similar Matricies
====================

|equivalence:
        2 matricies are equivalent if there are a pair of inveratble matricies [P₁ P₂] such that:
                P₁*A*P₂ = C

|similarity:                                                                    NOTE [E` == E^-1]
        2 matricies are similar if there is a matrix [P] such that:
                P*A*P` = C

        there is a special case if C is a diagonal matrix


6b-diagonizability
==================
consider:

        A ∈ R^[n×n]
        every column in [A] has a linearly independent EigenVector ([A] has [n] independent EigenVectors)
        
        for EigenVector [#_j]:
                A*∠v_j = λ_j*∠v_j
                
        E       = [∠v₁ ∠v₂ ... ∠v_n]            (the matrix of EigenVectors)

        A*E     = [λ₁∠v₁ λ₂∠v₂ ... λ_n*∠v_n]

                which can be restructured to be:
                = [∠v₁ ∠v₂ ... ∠v_n]  * /λ₁ 0  0  ⋯
                                        |0  λ₂ 0  ⋯
                                        |0  0  λ₂ ⋯
                                         ⋮  ⋮  ⋮
                which thus gives us the matrix of EigenVectors [E] and the matrix of EigenValues [Λ]
                = E*Λ 
        AE = EΛ 

so we can take this result [AE=EΛ]:

        AEE` = EΛE` ≡ A = EΛE`
which shows that [A] is a diagonizable matrix, as [E] is diagonal and [E] and [A] are similar
        E`AE = E`EΛ
             = Λ
which shows us how to get the diagonalization of [A], which is [Λ]

thus we have shown a sufficent condition that A is diagonizable:
        each eigenvalue in [E] must be unique
        that is to say, the eigen vectors are the basis vecotrs for the column space of [A]
        matrix A has an EigenVector basis 

the converse is not true:
        a matrix [A] without linearly independent eigenvalues does not show sufficient condition
        matrix A does not have EigenVector basis 


6c-diagonizable examples
========================
consider:
        A₁ =    / 1  4 \
               |        |
                \ 1 -2 /

        λ₁ = -3  λ₂ = +2

        to find [∠v_i] we could:
                - use RREF to find null(A - λ_i * I)
                - eig function in MatLab which returns the EigenValues and EigenVectors 

        ∠v₁ = [-1/√2; 1/√2]

        ∠v₁ ∝ [-1; 1]   
        ∠v₂ ∝ [ 4; 1]
        
        thus [A₁] is 

consider:
        A₂ =    / 1  4 \
               |        |
                \ 0 -2 /

        λ₁ = -2  λ₂ = +1

        ∠v₁ ∝ [-4; 3]
        ∠v₂ ∝ [ 1; 0]

consider:
        A₃ =    / 4  0  3 \  
               | -1  5 -3  |
                \ 2  0  11/

        λ₁ = λ₂ = 5     λ₃ = 10

       ∠v₁ ∝ [ 0; 1; 0]
       ∠v₂ ∝ [-3; 0; 1]
       ∠v₃ ∝ [ 1; 1;-2]


6d-Square root of a Matrix
==========================

|whole powers of a matrix:
        assuming [k] is a whole number
                A⁰      = I
                A^[k+1] = AA^k

        supose that [A] is diagonizable:
                A² = AA = EΛE`EΛE`
                        = EΛΛE`
                        = EΛ²E`
                A³ = AA²= EΛE`EΛ²E`
                        = EΛ³E` 

|square root of a Matrix:
        recall:
                [√a] is [a ∈ |R+ : a=c²] so [√a = c]
        extend:
                [A^½ = C] where [A = C²]

        for the diagonizable matrix [A], we can find the quare root by:

                A = EΛE` = C² = EDE     NOTE: [Λ == D²]
                                              [D == Λ^½]
                we find that:                              
                        [d_ii = √λ_i] is true if [λ_i ∈ |R+] or [λ_i ≥ 0]
                
                A^½ = EΛ^½E`


######
WEEK 3
######


Q1-review
=-=-=-=-=
- Graph Matrices
        > Graph [G(V,E)] with vertices [V] and edges [E]
        > in MatLab, V = (i ∈ ∥Z⁺ : i > 0), E = ((x,y) ∈ ∥Z⁺ : (x,y) ∈ V)
- Adjacency Matrix
        > a graph isomorphism is a permutation of the Adjacency matrix [A(G)]
        > degree of V in graph G(V,E) == [ D(G) == diag(A(G)*∠1) ]
- Laplacian Matrix
        > L(G) = D(G) - A(G)
        > diagnonally dominant and symmetric 
        > the # of components in G is equal to the nullspace of L(G)

- EigenValues & EigenVectors 


7-DOES NOT EXIST
================


8a-Normal Matricies
===================
consider:
        C ∈ R^[m×m]

for which:
        CC⁺ = C⁺C                               NOTE: A⁺ = A^T = A transposed
                                                      when A is a matrix  
such a matrix [C] is a NORMAL matrix
if [C] also satisfies these conditions it will also-
have the properties:
        
        C⁺ = C⁻¹        orthogonal
           = C          symmetric
           = -C         skew-symmetric

|EigenVector basis:
        recall that eigenvectors [∠v_j] of a diagonalizable matrix
        C ∈ ∥R^[m×m] are a basis for ∥R^m,
        thus:
                ∠c = α₁*∠v₁ + α₂*∠v₂ + ... + α_m*∠v_m

        each [α_i] is a component of the vector in the basis 


8b-Orthogonal Matrix
====================
given:
        orthogonal Matrix [Q]

        Q ∈ ∥R^[m×m]    the columns [∠q] are OrthoNormal

consider:
        A = Q⁺Q = [∠q⁺₁; ∠q⁺₂; ...; ∠q⁺_m][∠q₁,∠q₂,...,∠q_m]

enter:
        a_ij = [k=1→m]∑(q_ki*q_kj) = ∠q⁺_i * ∠q_j = ∠q_i * ∠q_j

        / i=j → ∠q_i*∠q_i = 1
        \ i≠j → ∠q_i*∠q_j = 0           because columns are OrthoNormal 

        A = Q⁺Q = I

which implies:
        Q⁻¹ = Q⁺

because only the inverse times the matrix equals the identity

|Orthogonal:
        2 vectors ∠u, ∠v
        vector subspaces \V, \W, \U
        matrix Q

|OrthoNormal:
        basis set of vectors

|example:
        Q ∈ ∥R^[2×2]

        Q = [cos(θ), -sin(θ); sin(θ), cos(θ)]

        EigenValues λ = cos(θ)±sin(θ)*√-1
                these are real iff [θ = kπ] for the int [k]


8c-real symmetric matrices
==========================
given:
        Symmetric Matrix [B]

        B ∈ ∥R^[m×m] : [b_ij = b_ji] ∨ [B = B⁺]

show:
        BB⁺ = B⁺B
        (its much easier to show BB⁺ = B⁺B)

        B = [1,2; 2,-2]
        λ₁= -3  λ₂= +2
        
        ∠v₁ ∝ [1; -2]   ∠v₂ ∝ [2; 1]
        
        thus:

        ∠v₁ ⊥ ∠v₂       [v1 is orthogonal to v2] 

this shows us that if a matrix is symmetric and real [B∈∥R]:
        - every EigenValue is real
        - if [λ_i ≠ λ_j] then [∠v_i * ∠v_j = 0] 
                > distingct EigenValues have orthogonal eigenvectors  

to prove that every eigenvalue is real in a symmetric matrix we must-
prove it with complex numbers
consider:
        [B∠v₁]·∠v₂ ≡ [λ₁∠v₁]·∠v₂ = λ₁(∠v₁·∠v₂)
        [B∠v₁]⁺∠v₂ = ∠v⁺₁B⁺∠v₂ = ∠v⁺₁B∠v₂
                   = ∠v⁺₁λ₂∠v₂
                   = λ₂(∠v₁·∠v₂)

        λ₁ ≠ λ₂ and λ₁(∠v₁·∠v₂) = λ₂(∠v₁·∠v₂)
                then ∠v₁·∠v₂ = 0

|Skew-Symmetric Martices:
        given:
                skew symmetric matrix [S]

                S ∈ ∥R^[m×m] : [s_ij = -s_ij] ∨ [S⁺ = -S]

        the EigenValues of [S] are either
                - zero [0]
                - purely imaginary [0+y√-1]

        the EigenVectors of [S] are generally complex


8d-Spectral Theorum
===================
an important fact in CISC271

suppose:
        normal matrix [ C ∈ ∥R^[m×m] ]

then:
        C = QΛQ⁺

where:
        - [Λ] is diagonal, [λ] is an eigenvalue of [C]
        - [Q] is Orthogonal 

|Symmetry Theorem:
        if a matrix is the same as it's transpose [B=B⁺], then
        - every eigenvalue is a real number
        - every eigenvector is composed entirely of real numbers


8-learning summary 
------------------
we should now be able to:
        identify a normal matrix as one that commutes with its transpose
        find the inverse of an orthogonal matrix by taking its transpose
        state the eigen-decomposition of a normal matrix using the spectral theorem
        characterize eigenvalues of skew symmetric matrix as being 0 or purely imaginary


9a-positive definite matrices
=============================

|Quadratic form of a vector:
        for any matrix [M], ∠u ≠ 0, the quadratic form of [M] is:

                ∠u⁺M∠u        
        also known as the 'energy' of a vector

consider:
        symmetric matrix [B]

        Positive Definite:      λ > 0   or      ∠u⁺B∠u > 0      or      B >) 0
        Positive Semi-Definite  λ ≥ 0   or      ∠u⁺B∠u ≥ 0      or      B ≥) 0

        Negative Definite       λ < 0   or      ∠u⁺B∠u < 0      or      B <) 0
        Negative Semi-Definite  λ ≤ 0   or      ∠u⁺M∠u ≤ 0      or      B ≤) 0

        Indefinite              none of the above


9b-Quadratic Form
=================
consider:
        Symmetric Matrix [ B ∈ ∥R^[m×m], B=B⁺ ]
        
quadratic:
        B = QΛQ⁺

basis of [∥R^m] must be ∠q_j in the quadratic form [q∈Q]

        Any  ∠u = α₁∠q₁+α₂∠q₂+...+α_n∠q_n
            B∠u = λ₁α₁∠q₁ + λ₂α₂∠q₂ + ... + λ_nα_n∠q_n
         ∠u⁺B∠u = λ₁α²₁∠q₁·∠q₁ + λ₂α²₂∠q₂·∠q₂ + ... + λ_nα²_n∠q_n·∠q_n  

                 = λ₁α²₁ + λ₂α²₂ + ... + λ_nα²_n 

each α²_j ≥ 0

if ∠u⁺B∠u > 0 then, generally, each λ > 0
if ∠u⁺B∠u ≥ 0 then, generally, each λ ≥ 0


consider:
        A ∈ ∥R^[m×n]
        with columns linearly independent

A must be full rank and either:                         NOTE: rank is the number of linearly independent columns
        square          n=m
        tall-thin       n>m

what does a [∠u ≠ 0] map to ?

        ∠y = A∠u

        ∥∠y∥² = ∠y⁺∠y
              = [A∠u]⁺A∠u
              = ∠u⁺A⁺A∠u        must be > 0
                      
thus, 
the symmetric (by construction) matrix [A⁺A] is Positive Definite

        [A⁺A] >) 0

|example:
        
        A   = [1,1; -1,1; 1,1]            [A] is full rank
        
        A⁺  = [1,-1,1; 1,1,1]             

        A⁺A = [3,1; 1,3]                  [A⁺A] is symmetric by construction

        λ₁  = 2          λ₂ = 4         

        ∠v₁ ∝ [1; -1]   ∠v₂ ∝ [1; 1]    

        observe that v₁ and v₂ (the eigenvectors of A) are-
        a basis for  ∥R^m


9c-Mean and Variance of data
============================
suppose we are given data [∠x ∥R^m]

the mean of [∠x]                ‾x              = 1/m × [j=1→m]∑(x_j)

difference vector [∠x]          ∠d              = [x₁ - ‾x₁; x₂ - ‾x₂; ...; x_m - ‾x_m] 
                                                = ∠x - ‾x∠1
                                                has [m-1] degrees of freedom

variance of [∠x]                var(∠x)         = 1/(m-1) × [j=1→m]∑(x_j - ‾x)²
                                                = 1/(m-1) × (∠d·∠d)

|mean:
        a = [1; 2; 3]   b = [4; 5; 6]

        ‾x_a = 2         ‾x_b = 5

        d_a = [-1; 0; 1]
        d_b = [-1; 0; 1]

|variance:
        a = [1; -1; 3]  b = [1; 3; 2]
        
        ‾x_a = 1        ‾x_b = 2

        d_a = [0; -2; 2]
        d_b = [-1; 1; 0]

        d_a·d_a = 8
        d_b·d_b = 2

        var(a) = 4
        var(b) = 1


9d-Covariance Matrix
====================
the covariance of 2 data vectors is 1 over the degrees of freedom times the dot product of the-
difference vectors:
        
        cov(∠x₁, ∠x₂)   = 1/(m-1) * (∠d₁·∠d₂)
                        = 1/(m-1) * (∠d⁺₁∠d₂)

|example:
        given:
                a = [1; -1; 3]  
                b = [1; 3; 2]

                ‾x_a = 1        
                ‾x_b = 2

                d_a = [0; -2; 2]
                d_b = [-1; 1; 0]

        find the covariance of [a] and [b]

        cov     = 1/(m-1) * (∠d⁺₁∠d₂)

                = 1/2[0,-2,2][-1; 1; 0]

                = 1/2(-2)

                = -1

from this covariance [-1] we can infer that in a deep sense that these data sets-
have a negative correlation which can be seen in the second entry


consider:
        X = [∠x₁ ... ∠x_n]              NOTE: [X] is a design matrix where all its columns are predefined vectors

then the covariance of [X] is:
                        
                        /∠d₁⁺∠d₁, ∠d₁⁺∠d₂, ..., ∠d₁⁺∠d_n\
        cov(X)= 1/(m-1) |∠d₂⁺∠d₁                        |
                        |  ...                          |
                        \∠d_n⁺∠d₁                       /

thus, the covariance of [X] can be simplified to

        cov(X)= 1/(m-1) * D⁺D                   NOTE: [D] is the difference matrix made up of difference vectors [∠d]

therefore, by construction the matrix [D⁺D] is symmetric and the- 
covariance of any design matrix must be symmetric and its covariance is 
positive semi-definite.

furthermore, if the difference vectors in the difference matrix [D],
then [D] is full rank and the covariance of [X] is positive definite.

NOTE
cov(X) is positive definite iff the vectors that make up [X] are statistically independent


9-learning outcomes
-------------------
we should now be able to:
        realate eigenvalues to quatratic form
        find positive definite matrix from full rank matrix and from a non full rank, positive semi-definite
        compute mean and variance of data vector
        formulate the covariance matrix of data vectors and if the data is statistically independent, it will make a positive semi definite matrix 
                
                
######
WEEK 4
######
